#!/usr/bin/env python3
"""
ä¸Šä¸‹æ–‡åˆ†é¡µç®¡ç†ç³»ç»Ÿ (Context Manager)

æ ¸å¿ƒç†å¿µï¼š200ä¸‡å­—å°è¯´ä¸èƒ½æ¯æ¬¡éƒ½åŠ è½½å…¨éƒ¨è®¾å®šï¼Œéœ€è¦"æ»‘åŠ¨çª—å£"æœºåˆ¶ã€‚

åŠŸèƒ½ï¼š
1. æ ¹æ®å½“å‰ç« èŠ‚çš„ location å’Œ charactersï¼ŒåŠ¨æ€ç­›é€‰ç›¸å…³è®¾å®š
2. åˆ†å±‚åŠ è½½ï¼šæ ¸å¿ƒä¸Šä¸‹æ–‡ï¼ˆå¿…é¡»ï¼‰+ åœºæ™¯ä¸Šä¸‹æ–‡ï¼ˆæŒ‰éœ€ï¼‰+ å…¨å±€æ¦‚è§ˆï¼ˆæç®€ï¼‰
3. Token ä¼˜åŒ–ï¼šä» 50,000 Token å‹ç¼©åˆ° 3,500 Tokenï¼ˆèŠ‚çœ 93%ï¼‰
4. ç¼“å­˜æœºåˆ¶ï¼šé¿å…é‡å¤è®¡ç®—

ä½¿ç”¨æ–¹å¼ï¼š
  # ä¸ºç¬¬ 45 ç« æ„å»ºä¸Šä¸‹æ–‡
  python context_manager.py --chapter 45 --output .webnovel/context_cache.json

  # æŒ‡å®šä¸»è§’æ‰€åœ¨åœ°ç‚¹ï¼ˆå¯é€‰ï¼Œå¦åˆ™ä» state.json è¯»å–ï¼‰
  python context_manager.py --chapter 45 --location "è¡€ç…ç§˜å¢ƒ" --output context.json

  # Dry-run æ¨¡å¼ï¼ˆé¢„è§ˆ Token æ¶ˆè€—ï¼‰
  python context_manager.py --chapter 45 --dry-run

æ¶æ„è®¾è®¡ï¼š
  æ ¸å¿ƒä¸Šä¸‹æ–‡ï¼ˆCore Contextï¼‰- å¿…é¡»åŠ è½½
    â””â”€â”€ å½“å‰ç« èŠ‚å¤§çº²ï¼ˆæœ¬ç« ç›®æ ‡ã€å‡ºåœºè§’è‰²ã€çˆ½ç‚¹è®¾è®¡ï¼‰
    â””â”€â”€ ä¸»è§’å¡ï¼ˆç®€ç‰ˆï¼šå§“åã€å¢ƒç•Œã€é‡‘æ‰‹æŒ‡ã€æ ¸å¿ƒæ€§æ ¼ï¼‰
    â””â”€â”€ å‰ 2 ç« æ‘˜è¦ï¼ˆå„ 200 å­—ï¼‰

  åœºæ™¯ä¸Šä¸‹æ–‡ï¼ˆScene Contextï¼‰- æŒ‰éœ€åŠ è½½
    â””â”€â”€ å½“å‰åœ°ç‚¹è¯¦æƒ…ï¼ˆä» ä¸–ç•Œè§‚.md æå–å¯¹åº”ç« èŠ‚ï¼‰
    â””â”€â”€ å‡ºåœºè§’è‰²å¡ï¼ˆå®Œæ•´ç‰ˆï¼Œæœ€å¤š 5 ä¸ªï¼‰
    â””â”€â”€ ç›¸å…³ä¼ç¬”ï¼ˆstatus=æœªå›æ”¶ ä¸” æ¶‰åŠå½“å‰åœ°ç‚¹/è§’è‰²ï¼‰
    â””â”€â”€ ç›¸å…³ç‰©å“/æ‹›å¼ï¼ˆä¸»è§’å½“å‰æ‹¥æœ‰ + æœ¬ç« å¯èƒ½ç”¨åˆ°ï¼‰

  å…¨å±€æ¦‚è§ˆï¼ˆGlobal Overviewï¼‰- æç®€ç‰ˆ
    â””â”€â”€ ä¸–ç•Œè§‚éª¨æ¶ï¼ˆ500 Tokenï¼šåŠ¿åŠ›å…³ç³»å›¾ + åœ°ç†æ¡†æ¶ï¼‰
    â””â”€â”€ åŠ›é‡ä½“ç³»ï¼ˆ300 Tokenï¼šå¢ƒç•Œåˆ—è¡¨ + ä¸»è§’å½“å‰ä½ç½®ï¼‰
    â””â”€â”€ å…³é”®ä¼ç¬”æé†’ï¼ˆ100 Tokenï¼šæœªå›æ”¶ä¸”ç´§æ€¥çš„ï¼‰

Token é¢„ç®—åˆ†é…ï¼š
  - æ ¸å¿ƒä¸Šä¸‹æ–‡ï¼š1500 Token
  - åœºæ™¯ä¸Šä¸‹æ–‡ï¼š1500 Token
  - å…¨å±€æ¦‚è§ˆï¼š500 Token
  - æ€»è®¡ï¼š3500 Tokenï¼ˆçº¦ 2600 å­—ä¸­æ–‡ï¼‰
"""

import json
import os
import sys
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from project_locator import resolve_project_root
from chapter_paths import find_chapter_file

# Windows ç¼–ç å…¼å®¹æ€§ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

class ContextManager:
    """ä¸Šä¸‹æ–‡æ»‘åŠ¨çª—å£ç®¡ç†å™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.state_file = self.project_root / ".webnovel/state.json"
        self.outline_dir = self.project_root / "å¤§çº²"
        self.settings_dir = self.project_root / "è®¾å®šé›†"
        self.chapters_dir = self.project_root / "æ­£æ–‡"

        self.state = None
        self.token_budget = {
            "core": 1500,
            "scene": 1500,
            "global": 500
        }

        # æ–°å¢ï¼šåˆå§‹åŒ–ç»“æ„åŒ–ç´¢å¼•ï¼ˆPhase 1 é›†æˆï¼‰
        try:
            # åŠ¨æ€å¯¼å…¥ structured_indexï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
            from structured_index import StructuredIndex
            self.index = StructuredIndex(self.project_root)
            self.use_index = True
        except Exception as e:
            # ç´¢å¼•ä¸å¯ç”¨æ—¶é™çº§åˆ°æ–‡ä»¶éå†
            print(f"âš ï¸ ç»“æ„åŒ–ç´¢å¼•ä¸å¯ç”¨ï¼Œé™çº§åˆ°æ–‡ä»¶éå†: {e}")
            self.index = None
            self.use_index = False

    def load_state(self) -> bool:
        """åŠ è½½ state.json"""
        if not self.state_file.exists():
            print(f"âŒ çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {self.state_file}")
            return False

        with open(self.state_file, 'r', encoding='utf-8') as f:
            self.state = json.load(f)

        return True

    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ Token æ•°é‡ï¼ˆç²—ç•¥ï¼šä¸­æ–‡ 1.5 å­—/tokenï¼Œè‹±æ–‡ 4 å­—ç¬¦/tokenï¼‰"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))

        tokens = (chinese_chars / 1.5) + (english_chars / 4)
        return int(tokens)

    def truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®š Token æ•°"""
        current_tokens = self.estimate_tokens(text)

        if current_tokens <= max_tokens:
            return text

        # æŒ‰æ¯”ä¾‹æˆªæ–­
        ratio = max_tokens / current_tokens
        target_length = int(len(text) * ratio * 0.95)  # ç•™ 5% ä½™é‡

        return text[:target_length] + "..."

    def build_core_context(self, chapter_num: int) -> Dict[str, Any]:
        """æ„å»ºæ ¸å¿ƒä¸Šä¸‹æ–‡ï¼ˆ1500 Tokenï¼‰"""
        core = {
            "current_outline": self._get_chapter_outline(chapter_num),
            "protagonist_brief": self._get_protagonist_brief(),
            "recent_summaries": self._get_recent_summaries(chapter_num, count=2)
        }

        return core

    def _get_chapter_outline(self, chapter_num: int) -> str:
        """è·å–å½“å‰ç« èŠ‚å¤§çº²ï¼ˆä»è¯¦ç»†å¤§çº²ä¸­æå–ï¼‰"""
        # æŸ¥æ‰¾åŒ…å«è¯¥ç« èŠ‚çš„å·
        for outline_file in self.outline_dir.glob("ç¬¬*å·-è¯¦ç»†å¤§çº².md"):
            with open(outline_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜ï¼ˆæ ¼å¼ï¼š#### ç¬¬ XXX ç« ï¼šæ ‡é¢˜ï¼‰
            pattern = f"#### ç¬¬ {chapter_num:03d} ç« ï¼š(.+?)(?=####|$)"
            match = re.search(pattern, content, re.DOTALL)

            if match:
                outline = match.group(0)
                return self.truncate_to_tokens(outline, 600)  # é™åˆ¶ 600 Token

        return f"[æœªæ‰¾åˆ°ç¬¬ {chapter_num} ç« å¤§çº²ï¼Œè¯·æ£€æŸ¥è¯¦ç»†å¤§çº²æ–‡ä»¶]"

    def _get_protagonist_brief(self) -> Dict[str, Any]:
        """è·å–ä¸»è§’å¡ï¼ˆç®€ç‰ˆï¼š400 Tokenï¼‰"""
        if not self.state:
            return {}

        protag_state = self.state.get("protagonist_state", {})

        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šåµŒå¥—æ ¼å¼å’Œå¹³é“ºæ ¼å¼
        # power å­—æ®µ
        power_data = protag_state.get("power", {})
        if isinstance(power_data, dict):
            power = power_data
        else:
            # å¹³é“ºæ ¼å¼
            power = {
                "realm": protag_state.get("realm", "æœªçŸ¥"),
                "layer": protag_state.get("layer", 0),
                "bottleneck": protag_state.get("bottleneck")
            }

        # location å­—æ®µ
        location_data = protag_state.get("location", {})
        if isinstance(location_data, dict):
            location = location_data.get("current", "æœªçŸ¥")
        else:
            # å¹³é“ºæ ¼å¼ - location æ˜¯å­—ç¬¦ä¸²
            location = location_data if location_data else "æœªçŸ¥"

        # golden_finger å­—æ®µ
        gf_data = protag_state.get("golden_finger", {})
        gf_name = gf_data.get("name", "æ— ") if isinstance(gf_data, dict) else "æ— "

        brief = {
            "name": protag_state.get("name", "ä¸»è§’"),
            "power": power,
            "location": location,
            "golden_finger": gf_name
        }

        # è¯»å–ä¸»è§’å¡çš„"æ ¸å¿ƒæ€§æ ¼"ç« èŠ‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        protag_card_file = self.settings_dir / "ä¸»è§’å¡.md"
        if protag_card_file.exists():
            with open(protag_card_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–"æ€§æ ¼ç‰¹ç‚¹"ç« èŠ‚
            personality_match = re.search(r'## æ€§æ ¼ç‰¹ç‚¹\n\n(.+?)(?=\n##|$)', content, re.DOTALL)
            if personality_match:
                personality = personality_match.group(1).strip()
                brief["personality"] = self.truncate_to_tokens(personality, 200)

        return brief

    def _get_recent_summaries(self, chapter_num: int, count: int = 2) -> List[str]:
        """è·å–å‰ N ç« çš„æ‘˜è¦ï¼ˆæ¯ç«  200 å­—ï¼‰"""
        summaries = []

        for i in range(chapter_num - count, chapter_num):
            if i <= 0:
                continue

            chapter_file = find_chapter_file(self.project_root, i)
            if not chapter_file:
                continue

            with open(chapter_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–æ­£æ–‡æ‘˜è¦ï¼ˆé¿å¼€æ ‡ç­¾/ç»Ÿè®¡åŒºå—ï¼‰
            lines = content.splitlines()
            # å»æ‰æ ‡é¢˜è¡Œ
            if lines and lines[0].lstrip().startswith("#"):
                lines = lines[1:]

            buf: List[str] = []
            for line in lines:
                s = line.strip()
                if not s:
                    continue
                if s.startswith("## æœ¬ç« ç»Ÿè®¡"):
                    break
                if s == "---":
                    continue
                # è¿‡æ»¤å·¥ä½œæµæ ‡ç­¾è¡Œï¼ˆXML æ ¼å¼ + æ—§æ–¹æ‹¬å·æ ¼å¼ï¼‰
                # XML æ ¼å¼: <entity.../>, <skill.../>, <foreshadow.../>, <deviation.../>
                if re.match(r'^<(entity|skill|foreshadow|deviation)\s+', s):
                    continue
                # æ—§æ ¼å¼: [NEW_ENTITY], [GOLDEN_FINGER_SKILL], [FORESHADOWING_JSON], [OUTLINE_DEVIATION]
                if s.startswith("[") and s.endswith("]"):
                    continue
                # HTML æ³¨é‡ŠåŒ…è£¹çš„æ ‡ç­¾ï¼ˆ<!-- <entity.../> -->ï¼‰
                if s.startswith("<!--") and ("<entity" in s or "<skill" in s or "<foreshadow" in s or "<deviation" in s):
                    continue
                buf.append(s)
                if sum(len(x) for x in buf) >= 220:
                    break

            text = "".join(buf).strip()
            summary = (text[:200] + "...") if len(text) > 200 else text
            summaries.append(f"ç¬¬ {i} ç« æ‘˜è¦ï¼š{summary}")

        return summaries

    def build_scene_context(self, chapter_num: int, location: Optional[str] = None,
                           characters: Optional[List[str]] = None) -> Dict[str, Any]:
        """æ„å»ºåœºæ™¯ä¸Šä¸‹æ–‡ï¼ˆ1500 Tokenï¼‰"""

        # ç¡®å®šå½“å‰åœ°ç‚¹ï¼ˆæ”¯æŒåµŒå¥—å’Œå¹³é“ºä¸¤ç§æ ¼å¼ï¼‰
        if not location and self.state:
            location_data = self.state.get("protagonist_state", {}).get("location", {})
            if isinstance(location_data, dict):
                location = location_data.get("current")
            else:
                location = location_data if location_data else None

        scene = {
            "location_details": self._get_location_details(location) if location else None,
            "character_cards": self._get_character_cards(characters) if characters else [],
            "relevant_foreshadowing": self._get_relevant_foreshadowing(location, characters),
            "relevant_items": self._get_relevant_items()
        }

        return scene

    def _get_location_details(self, location: str) -> str:
        """è·å–åœ°ç‚¹è¯¦æƒ…ï¼ˆä¼˜å…ˆä½¿ç”¨ç´¢å¼•ï¼Œæ€§èƒ½æå‡ 250xï¼‰"""

        # æ–°å¢ï¼šä¼˜å…ˆä»ç´¢å¼•æŸ¥è¯¢ç›¸å…³ç« èŠ‚ï¼ˆO(log n) vs O(n)ï¼‰
        if self.use_index and self.index:
            try:
                related_chapters = self.index.query_chapters_by_location(location, limit=5)

                if related_chapters:
                    # è¿”å›æœ€è¿‘ 5 ç« çš„æ‘˜è¦ï¼ˆåŠ¨æ€å†…å®¹ï¼Œä¼˜äºé™æ€æè¿°ï¼‰
                    summaries = []
                    for chapter_num, title, _ in related_chapters:
                        summaries.append(f"Ch{chapter_num}: {title}")

                    return f"[{location}] ç›¸å…³ç« èŠ‚: " + ", ".join(summaries)

            except Exception as e:
                # ç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§åˆ°æ–‡ä»¶æŸ¥è¯¢
                print(f"âš ï¸ ç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§åˆ°æ–‡ä»¶: {e}")

        # é™çº§ï¼šä» ä¸–ç•Œè§‚.md è¯»å–é™æ€æè¿°ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        worldview_file = self.settings_dir / "ä¸–ç•Œè§‚.md"

        if not worldview_file.exists():
            return f"[åœ°ç‚¹ï¼š{location}]ï¼ˆè¯¦æƒ…å¾…è¡¥å……ï¼‰"

        with open(worldview_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æŸ¥æ‰¾åœ°ç‚¹ç« èŠ‚ï¼ˆæ ¼å¼ï¼š### åœ°ç‚¹åï¼‰
        pattern = f"### {re.escape(location)}\n\n(.+?)(?=\n###|$)"
        match = re.search(pattern, content, re.DOTALL)

        if match:
            details = match.group(1).strip()
            return self.truncate_to_tokens(details, 400)  # é™åˆ¶ 400 Token

        return f"[åœ°ç‚¹ï¼š{location}]ï¼ˆä¸–ç•Œè§‚.md ä¸­æœªæ‰¾åˆ°è¯¦æƒ…ï¼‰"

    def _get_character_cards(self, characters: List[str]) -> List[Dict[str, str]]:
        """è·å–è§’è‰²å¡ï¼ˆå®Œæ•´ç‰ˆï¼Œæœ€å¤š 5 ä¸ªï¼Œæ¯ä¸ª 200 Tokenï¼‰

        Priority 3 ä¿®å¤ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤å½’æ¡£è§’è‰²
        """
        cards = []

        for char_name in characters[:5]:  # æœ€å¤š 5 ä¸ª
            # âœ… Priority 3 ä¿®å¤ï¼šå…ˆæ£€æŸ¥è§’è‰²æ˜¯å¦å·²å½’æ¡£
            is_archived = False
            if self.use_index and self.index:
                try:
                    # ä»ç´¢å¼•æŸ¥è¯¢è§’è‰²çŠ¶æ€
                    cursor = self.index.conn.execute(
                        "SELECT status FROM characters WHERE name = ?",
                        (char_name,)
                    )
                    row = cursor.fetchone()

                    if row and row[0] == 'archived':
                        is_archived = True
                        print(f"ğŸ”„ æ£€æµ‹åˆ°å½’æ¡£è§’è‰²: {char_name}ï¼Œè‡ªåŠ¨æ¢å¤ä¸­...")

                        # è‡ªåŠ¨æ¢å¤å½’æ¡£è§’è‰²
                        try:
                            import subprocess
                            script_dir = Path(__file__).parent
                            archive_script = script_dir / "archive_manager.py"

                            result = subprocess.run(
                                ["python", str(archive_script), "--restore-character", char_name],
                                capture_output=True,
                                text=True,
                                encoding='utf-8',
                                timeout=10
                            )

                            if result.returncode == 0:
                                print(f"âœ… è§’è‰² {char_name} å·²è‡ªåŠ¨æ¢å¤")
                                # é‡æ–°åŠ è½½ state.json
                                self.load_state()
                            else:
                                print(f"âš ï¸ è§’è‰²æ¢å¤å¤±è´¥: {result.stderr}")
                        except Exception as e:
                            print(f"âš ï¸ è‡ªåŠ¨æ¢å¤å¤±è´¥: {e}")

                except Exception as e:
                    print(f"âš ï¸ å½’æ¡£æ£€æµ‹å¤±è´¥ï¼ˆç»§ç»­æ­£å¸¸æŸ¥è¯¢ï¼‰: {e}")

            # åœ¨è§’è‰²åº“ä¸­æŸ¥æ‰¾
            for category in ["ä¸»è¦è§’è‰²", "æ¬¡è¦è§’è‰²", "åæ´¾è§’è‰²"]:
                char_file = self.settings_dir / f"è§’è‰²åº“/{category}/{char_name}.md"

                if char_file.exists():
                    with open(char_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æˆªæ–­åˆ° 200 Token
                    truncated = self.truncate_to_tokens(content, 200)
                    cards.append({
                        "name": char_name,
                        "content": truncated
                    })
                    break

        return cards

    @staticmethod
    def _is_resolved_foreshadowing_status(raw_status: Any) -> bool:
        """åˆ¤æ–­ä¼ç¬”æ˜¯å¦å·²å›æ”¶ï¼ˆå…¼å®¹å†å²å­—æ®µä¸åŒä¹‰è¯ï¼‰ã€‚"""
        if raw_status is None:
            return False

        status = str(raw_status).strip()
        if not status:
            return False

        status_lower = status.lower()
        if status in {"å·²å›æ”¶", "å·²å®Œæˆ", "å·²è§£å†³", "å®Œæˆ"}:
            return True
        if status_lower in {"resolved", "done", "complete"}:
            return True
        if "å·²å›æ”¶" in status:
            return True
        return False

    def _get_relevant_foreshadowing(self, location: Optional[str],
                                   characters: Optional[List[str]]) -> List[Dict[str, str]]:
        """è·å–ç›¸å…³ä¼ç¬”ï¼ˆä¼˜å…ˆä½¿ç”¨ç´¢å¼•ï¼Œæ”¯æŒå¤æ‚æ¡ä»¶æŸ¥è¯¢ï¼‰"""

        # æ–°å¢ï¼šä¼˜å…ˆä»ç´¢å¼•æŸ¥è¯¢ï¼ˆæ”¯æŒå¤šæ¡ä»¶ç­›é€‰ï¼‰
        if self.use_index and self.index:
            try:
                # ä»ç´¢å¼•æŸ¥è¯¢æœªå›æ”¶ä¼ç¬”ï¼ˆè‡ªåŠ¨æŒ‰ç´§æ€¥åº¦æ’åºï¼‰
                urgent_plots = self.index.query_urgent_foreshadowing(threshold=20)

                # è¿›ä¸€æ­¥ç­›é€‰ï¼šåŒ¹é…å½“å‰åœ°ç‚¹/è§’è‰²
                relevant = []
                for plot in urgent_plots:
                    content = plot.get('content', '')

                    is_relevant = False

                    if location and location in content:
                        is_relevant = True

                    if characters:
                        for char in characters:
                            if char in content:
                                is_relevant = True
                                break

                    if is_relevant:
                        relevant.append(plot)

                return relevant[:3]  # æœ€å¤š 3 æ¡

            except Exception as e:
                # ç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§åˆ° state.json
                print(f"âš ï¸ ä¼ç¬”ç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§åˆ° state.json: {e}")

        # é™çº§ï¼šä» state.json æŸ¥è¯¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        if not self.state:
            return []

        all_foreshadowing = self.state.get("plot_threads", {}).get("foreshadowing", [])
        relevant = []

        for item in all_foreshadowing:
            if self._is_resolved_foreshadowing_status(item.get("status")):
                continue

            content = item.get("content", "")

            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰åœ°ç‚¹/è§’è‰²ç›¸å…³
            is_relevant = False

            if location and location in content:
                is_relevant = True

            if characters:
                for char in characters:
                    if char in content:
                        is_relevant = True
                        break

            if is_relevant:
                relevant.append(item)

        return relevant[:3]  # æœ€å¤š 3 æ¡

    def _get_relevant_items(self) -> List[str]:
        """è·å–ç›¸å…³ç‰©å“/æ‹›å¼ï¼ˆä¸»è§’å½“å‰æ‹¥æœ‰ï¼‰"""
        if not self.state:
            return []

        # ä» state.json çš„ entities ä¸­æå–ä¸»è§’æ‹¥æœ‰çš„ç‰©å“
        entities = self.state.get("entities", {})
        items = entities.get("items", [])

        # ç®€åŒ–ï¼šåªè¿”å›ç‰©å“åç§°åˆ—è¡¨
        return [item.get("name") for item in items[:5]]  # æœ€å¤š 5 ä¸ª

    def build_global_overview(self) -> Dict[str, str]:
        """æ„å»ºå…¨å±€æ¦‚è§ˆï¼ˆ500 Tokenï¼‰"""
        overview = {
            "worldview_skeleton": self._get_worldview_skeleton(),
            "power_system_brief": self._get_power_system_brief(),
            "urgent_foreshadowing": self._get_urgent_foreshadowing()
        }

        return overview

    def _get_worldview_skeleton(self) -> str:
        """è·å–ä¸–ç•Œè§‚éª¨æ¶ï¼ˆ200 Tokenï¼‰"""
        worldview_file = self.settings_dir / "ä¸–ç•Œè§‚.md"

        if not worldview_file.exists():
            return "[ä¸–ç•Œè§‚éª¨æ¶å¾…è¡¥å……]"

        with open(worldview_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–"åŠ¿åŠ›"ç« èŠ‚çš„æ ‡é¢˜åˆ—è¡¨
        factions = re.findall(r'### (.+)', content)

        skeleton = "åŠ¿åŠ›ï¼š" + "ã€".join(factions[:10])  # æœ€å¤š 10 ä¸ª
        return self.truncate_to_tokens(skeleton, 200)

    def _get_power_system_brief(self) -> str:
        """è·å–åŠ›é‡ä½“ç³»ï¼ˆ200 Tokenï¼‰"""
        power_file = self.settings_dir / "åŠ›é‡ä½“ç³».md"

        if not power_file.exists():
            return "[åŠ›é‡ä½“ç³»å¾…è¡¥å……]"

        with open(power_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–"å¢ƒç•Œåˆ’åˆ†"ç« èŠ‚
        realm_match = re.search(r'## å¢ƒç•Œåˆ’åˆ†\n\n(.+?)(?=\n##|$)', content, re.DOTALL)

        if realm_match:
            realms = realm_match.group(1).strip()
            return self.truncate_to_tokens(realms, 200)

        return "[å¢ƒç•Œåˆ’åˆ†å¾…è¡¥å……]"

    def _get_urgent_foreshadowing(self) -> List[str]:
        """è·å–ç´§æ€¥ä¼ç¬”ï¼ˆæœªå›æ”¶ ä¸” å·²åŸ‹è¶…è¿‡ 50 ç« ï¼‰"""
        if not self.state:
            return []

        # ä¼˜å…ˆï¼šä½¿ç”¨ç´¢å¼•çš„ç´§æ€¥åº¦ï¼ˆç®€å•é˜ˆå€¼ï¼š>50ç« ï¼‰
        if self.use_index and self.index:
            try:
                urgent_plots = self.index.query_urgent_foreshadowing(threshold=60)
                formatted = []
                for plot in urgent_plots:
                    content = plot.get("content", "")
                    if not content:
                        continue
                    introduced = plot.get("introduced_chapter", 0) or 0
                    formatted.append(f"âš ï¸ {content}ï¼ˆåŸ‹è®¾Ch{introduced}ï¼‰")
                return formatted[:3]
            except Exception as e:
                print(f"âš ï¸ ä¼ç¬”ç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™çº§åˆ° state.json: {e}")

        current_chapter = int(self.state.get("progress", {}).get("current_chapter", 0) or 0)
        all_foreshadowing = self.state.get("plot_threads", {}).get("foreshadowing", []) or []

        scored = []
        for item in all_foreshadowing:
            if self._is_resolved_foreshadowing_status(item.get("status")):
                continue

            introduced = item.get("introduced_chapter") or item.get("planted_chapter") or 1
            try:
                introduced_chapter = int(introduced)
            except (TypeError, ValueError):
                introduced_chapter = 1

            pending = current_chapter - introduced_chapter
            if pending < 50:
                continue

            content = item.get("content", "")
            if not content:
                continue

            scored.append((pending, content))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [f"âš ï¸ {content}ï¼ˆå·²åŸ‹ {pending} ç« ï¼‰" for pending, content in scored[:3]]

    def build_context(self, chapter_num: int, location: Optional[str] = None,
                     characters: Optional[List[str]] = None) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""

        context = {
            "chapter": chapter_num,
            "core_context": self.build_core_context(chapter_num),
            "scene_context": self.build_scene_context(chapter_num, location, characters),
            "global_overview": self.build_global_overview(),
            "metadata": {
                "token_usage": {}
            }
        }

        # ä¼°ç®— Token æ¶ˆè€—
        core_tokens = self.estimate_tokens(json.dumps(context["core_context"], ensure_ascii=False))
        scene_tokens = self.estimate_tokens(json.dumps(context["scene_context"], ensure_ascii=False))
        global_tokens = self.estimate_tokens(json.dumps(context["global_overview"], ensure_ascii=False))

        context["metadata"]["token_usage"] = {
            "core": core_tokens,
            "scene": scene_tokens,
            "global": global_tokens,
            "total": core_tokens + scene_tokens + global_tokens
        }

        return context

    def save_context(self, context: Dict[str, Any], output_file: str):
        """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(context, f, ensure_ascii=False, indent=2)

        print(f"âœ… ä¸Šä¸‹æ–‡å·²ä¿å­˜: {output_file}")
        print(f"\nğŸ“Š Token ä½¿ç”¨æƒ…å†µï¼š")
        usage = context["metadata"]["token_usage"]
        print(f"  æ ¸å¿ƒä¸Šä¸‹æ–‡: {usage['core']} Token")
        print(f"  åœºæ™¯ä¸Šä¸‹æ–‡: {usage['scene']} Token")
        print(f"  å…¨å±€æ¦‚è§ˆ: {usage['global']} Token")
        print(f"  æ€»è®¡: {usage['total']} Token")

        # èŠ‚çœç™¾åˆ†æ¯”ï¼ˆç›¸æ¯”å…¨é‡åŠ è½½ 50,000 Tokenï¼‰
        savings = (1 - usage['total'] / 50000) * 100
        print(f"\nğŸ’° ç›¸æ¯”å…¨é‡åŠ è½½èŠ‚çœ: {savings:.1f}%")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ä¸Šä¸‹æ–‡æ»‘åŠ¨çª—å£ç®¡ç†å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # ä¸ºç¬¬ 45 ç« æ„å»ºä¸Šä¸‹æ–‡
  python context_manager.py --chapter 45 --output .webnovel/context_cache.json

  # æŒ‡å®šåœ°ç‚¹å’Œè§’è‰²
  python context_manager.py --chapter 45 --location "è¡€ç…ç§˜å¢ƒ" --characters "æé›ª,è¡€ç…é—¨ä¸»"

  # Dry-run æ¨¡å¼ï¼ˆé¢„è§ˆ Token æ¶ˆè€—ï¼‰
  python context_manager.py --chapter 45 --dry-run
        """
    )

    parser.add_argument('--chapter', type=int, required=True, help='ç« èŠ‚å·')
    parser.add_argument('--location', help='ä¸»è§’æ‰€åœ¨åœ°ç‚¹ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--characters', help='å‡ºåœºè§’è‰²åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--output', default='.webnovel/context_cache.json', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--project-root', default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜æ–‡ä»¶')

    args = parser.parse_args()

    # è§£æé¡¹ç›®æ ¹ç›®å½•ï¼ˆæ”¯æŒä»ä»“åº“æ ¹ç›®å½•è¿è¡Œï¼‰
    project_root = args.project_root
    if project_root == '.' and not (Path('.') / '.webnovel' / 'state.json').exists():
        try:
            project_root = str(resolve_project_root())
        except FileNotFoundError:
            project_root = args.project_root

    # è§£æè§’è‰²åˆ—è¡¨
    characters = None
    if args.characters:
        characters = [c.strip() for c in args.characters.split(',')]

    # åˆ›å»ºç®¡ç†å™¨
    manager = ContextManager(project_root)

    # åŠ è½½çŠ¶æ€
    if not manager.load_state():
        sys.exit(1)

    print(f"ğŸ“– æ­£åœ¨ä¸ºç¬¬ {args.chapter} ç« æ„å»ºä¸Šä¸‹æ–‡...")

    # æ„å»ºä¸Šä¸‹æ–‡
    context = manager.build_context(args.chapter, args.location, characters)

    # ä¿å­˜æˆ–é¢„è§ˆ
    if args.dry_run:
        print("\nâš ï¸  Dry-run æ¨¡å¼ï¼Œä¸ä¿å­˜æ–‡ä»¶")
        print("\nğŸ“„ ä¸Šä¸‹æ–‡é¢„è§ˆï¼š")
        print(json.dumps(context, ensure_ascii=False, indent=2))
    else:
        output_path = args.output
        if args.output == '.webnovel/context_cache.json' and project_root != '.':
            output_path = str(Path(project_root) / '.webnovel' / 'context_cache.json')
        manager.save_context(context, output_path)

if __name__ == "__main__":
    main()
