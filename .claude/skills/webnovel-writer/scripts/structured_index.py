#!/usr/bin/env python3
"""
ç»“æ„åŒ–ç´¢å¼•ç³»ç»Ÿï¼ˆStructured Index Systemï¼‰

ç›®æ ‡ï¼šå–ä»£å‘é‡åŒ–æ£€ç´¢ï¼Œä½¿ç”¨ SQLite æä¾›ç²¾ç¡®ã€å¿«é€Ÿçš„ç»“æ„åŒ–æŸ¥è¯¢

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ç« èŠ‚å…ƒæ•°æ®ç´¢å¼•ï¼ˆlocation, characters, word_countï¼‰
2. ä¼ç¬”è¿½è¸ªç´¢å¼•ï¼ˆstatus, urgency calculationï¼‰
3. æ–‡ä»¶ Hash è‡ªæ„ˆæœºåˆ¶ï¼ˆauto-rebuild on changeï¼‰

æ€§èƒ½ç›®æ ‡ï¼š
- æŸ¥è¯¢é€Ÿåº¦ï¼š2-5msï¼ˆvs æ–‡ä»¶éå† 500msï¼Œæå‡ 250xï¼‰
- ç´¢å¼•æ„å»ºï¼š10ms/ç« ï¼ˆå¢é‡æ›´æ–°ï¼‰
- å­˜å‚¨å¼€é”€ï¼š200 ç«  â‰ˆ 100 KB

ä½¿ç”¨æ–¹å¼ï¼š
  # æ›´æ–°å•ç« ç´¢å¼•
  python structured_index.py --update-chapter 7 --metadata "æ­£æ–‡/ç¬¬0007ç« .md"

  # æ‰¹é‡é‡å»ºç´¢å¼•ï¼ˆå†å²ç« èŠ‚ï¼‰
  python structured_index.py --rebuild-index

  # æŸ¥è¯¢åœ°ç‚¹ç›¸å…³ç« èŠ‚
  python structured_index.py --query-location "è¡€ç…ç§˜å¢ƒ"

  # æŸ¥è¯¢ç´§æ€¥ä¼ç¬”
  python structured_index.py --query-urgent-foreshadowing

  # æ¨¡ç³ŠæŸ¥è¯¢è§’è‰²
  python structured_index.py --fuzzy-search "å§“æ" "å¥³å¼Ÿå­"

  # å¯¼å‡ºå…³ç³»å›¾
  python structured_index.py --export-graph > relationships.md
"""

import json
import os
import sys
import argparse
import sqlite3
import hashlib
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Tuple


class StructuredIndex:
    """ç»“æ„åŒ–ç´¢å¼•ç®¡ç†å™¨ï¼ˆå–ä»£å‘é‡åŒ–æ£€ç´¢ï¼‰"""

    def __init__(self, project_root=None):
        if project_root is None:
            project_root = Path.cwd()
        else:
            project_root = Path(project_root)

        self.project_root = project_root
        self.state_file = project_root / ".webnovel" / "state.json"
        self.chapters_dir = project_root / "æ­£æ–‡"
        self.index_db = project_root / ".webnovel" / "index.db"

        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        self.index_db.parent.mkdir(parents=True, exist_ok=True)

        # è¿æ¥æ•°æ®åº“
        self.conn = sqlite3.connect(str(self.index_db))
        self.conn.row_factory = sqlite3.Row  # è¿”å›å­—å…¸å¼è¡Œ

        # åˆ›å»ºè¡¨ç»“æ„
        self._create_tables()

    def _create_tables(self):
        """åˆ›å»ºç´¢å¼•è¡¨ç»“æ„"""

        # 1. ç« èŠ‚å…ƒæ•°æ®è¡¨
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS chapters (
                chapter_num INTEGER PRIMARY KEY,
                title TEXT,
                location TEXT,
                characters TEXT,  -- JSON: ["æé›ª", "ä¸»è§’"]
                word_count INTEGER,
                content_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # åœ°ç‚¹ç´¢å¼•ï¼ˆåŠ é€ŸæŸ¥è¯¢ï¼‰
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_location
            ON chapters(location)
        """)

        # 2. ä¼ç¬”è¿½è¸ªè¡¨
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS foreshadowing_index (
                id INTEGER PRIMARY KEY,
                content TEXT,
                location TEXT,
                characters TEXT,  -- JSON: ["æé›ª", "ä¸»è§’"]
                introduced_chapter INTEGER,
                resolved_chapter INTEGER,
                status TEXT,  -- 'æœªå›æ”¶' / 'å·²å›æ”¶'
                urgency INTEGER DEFAULT 0,  -- 0-100ï¼Œè‡ªåŠ¨è®¡ç®—
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # çŠ¶æ€ç´¢å¼•
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_status
            ON foreshadowing_index(status)
        """)

        # ç´§æ€¥åº¦ç´¢å¼•
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_urgency
            ON foreshadowing_index(urgency)
        """)

        # 3. è§’è‰²å…³ç³»è¡¨
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS relationships (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                char1 TEXT,
                char2 TEXT,
                relation_type TEXT,  -- 'ally', 'enemy', 'romance', 'mentor', 'debtor'
                intensity INTEGER,    -- å…³ç³»å¼ºåº¦ 0-100
                description TEXT,
                last_update_chapter INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(char1, char2, relation_type)  -- é˜²æ­¢é‡å¤
            )
        """)

        # å…³ç³»ç´¢å¼•
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_char1_char2
            ON relationships(char1, char2)
        """)

        self.conn.commit()

    # ================== æ ¸å¿ƒåŠŸèƒ½ 1ï¼šç« èŠ‚å…ƒæ•°æ®ç´¢å¼• ==================

    def index_chapter(self, chapter_num: int, metadata: Dict):
        """ä¸ºæ–°ç« èŠ‚å»ºç«‹ç´¢å¼•ï¼ˆåœ¨ webnovel-write Step 4.6 è°ƒç”¨ï¼‰

        Args:
            chapter_num: ç« èŠ‚ç¼–å·
            metadata: {
                'title': 'ç« èŠ‚æ ‡é¢˜',
                'location': 'åœ°ç‚¹',
                'characters': ['æé›ª', 'ä¸»è§’'],
                'word_count': 3500,
                'hash': 'md5_hash'
            }
        """
        self.conn.execute("""
            INSERT OR REPLACE INTO chapters
            (chapter_num, title, location, characters, word_count, content_hash, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """, (
            chapter_num,
            metadata['title'],
            metadata['location'],
            json.dumps(metadata['characters'], ensure_ascii=False),
            metadata['word_count'],
            metadata['hash']
        ))

        self.conn.commit()
        print(f"âœ… ç« èŠ‚ç´¢å¼•å·²æ›´æ–°ï¼šCh{chapter_num} - {metadata['title']}")

    def query_chapters_by_location(self, location: str, limit: int = 10) -> List[Tuple]:
        """O(log n) æŸ¥è¯¢ï¼šè¿”å›è¯¥åœ°ç‚¹çš„æœ€è¿‘ N ç« 

        Args:
            location: åœ°ç‚¹åç§°
            limit: è¿”å›æ•°é‡

        Returns:
            [(chapter_num, title, characters), ...]
        """
        cursor = self.conn.execute("""
            SELECT chapter_num, title, characters
            FROM chapters
            WHERE location = ?
            ORDER BY chapter_num DESC
            LIMIT ?
        """, (location, limit))

        return cursor.fetchall()

    def calculate_chapter_hash(self, chapter_file: Path) -> str:
        """è®¡ç®—ç« èŠ‚æ–‡ä»¶ MD5 Hashï¼ˆç”¨äºè‡ªæ„ˆæœºåˆ¶ï¼‰"""
        if not chapter_file.exists():
            return ""

        with open(chapter_file, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def get_stored_hash(self, chapter_num: int) -> Optional[str]:
        """ä»ç´¢å¼•ä¸­è¯»å–å­˜å‚¨çš„ Hash"""
        cursor = self.conn.execute("""
            SELECT content_hash FROM chapters WHERE chapter_num = ?
        """, (chapter_num,))

        row = cursor.fetchone()
        return row['content_hash'] if row else None

    def validate_and_rebuild_if_needed(self, chapter_num: int):
        """æ ¡éªŒç« èŠ‚ Hashï¼Œä¸ä¸€è‡´åˆ™è‡ªåŠ¨é‡å»ºç´¢å¼•ï¼ˆSelf-Healing Indexï¼‰

        è§¦å‘æ—¶æœºï¼š
        - context_manager.py æŸ¥è¯¢ç« èŠ‚å‰è°ƒç”¨
        - å¢åŠ è€—æ—¶ï¼š~5msï¼ˆHash è®¡ç®— + å¯¹æ¯”ï¼‰
        - ä»…å½“æ£€æµ‹åˆ°å˜æ›´æ—¶æ‰é‡å»ºï¼ˆå¢é‡æˆæœ¬ï¼‰
        """
        chapter_file = self.chapters_dir / f"ç¬¬{chapter_num:04d}ç« .md"

        if not chapter_file.exists():
            return  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡

        # è®¡ç®—å½“å‰æ–‡ä»¶ Hash
        current_hash = self.calculate_chapter_hash(chapter_file)

        # ä»ç´¢å¼•ä¸­è¯»å–å­˜å‚¨çš„ Hash
        stored_hash = self.get_stored_hash(chapter_num)

        if current_hash != stored_hash:
            print(f"âš ï¸ æ£€æµ‹åˆ° Ch{chapter_num} å·²ä¿®æ”¹ï¼Œè‡ªåŠ¨é‡å»ºç´¢å¼•...")
            self._rebuild_chapter_index(chapter_num, chapter_file)
            print(f"âœ… Ch{chapter_num} ç´¢å¼•å·²æ›´æ–°")

    def _rebuild_chapter_index(self, chapter_num: int, chapter_file: Path):
        """é‡å»ºå•ç« ç´¢å¼•ï¼ˆè‡ªåŠ¨æå–å…ƒæ•°æ®ï¼‰"""

        # è¯»å–ç« èŠ‚å†…å®¹
        with open(chapter_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–å…ƒæ•°æ®
        metadata = self._extract_metadata_from_content(content, chapter_num)

        # é‡å»ºç´¢å¼•
        self.index_chapter(chapter_num, metadata)

    def _extract_metadata_from_content(self, content: str, chapter_num: int) -> Dict:
        """ä»ç« èŠ‚å†…å®¹ä¸­æå–å…ƒæ•°æ®"""

        # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
        lines = content.split('\n')
        title = lines[0].strip('# ').strip() if lines else f"ç¬¬{chapter_num}ç« "

        # æå–åœ°ç‚¹ï¼ˆåœ¨ç« èŠ‚å¼€å¤´æŸ¥æ‰¾ï¼Œé€šå¸¸æ ¼å¼ä¸º **åœ°ç‚¹ï¼šXXX**ï¼‰
        location_match = re.search(r'\*\*åœ°ç‚¹[ï¼š:]\s*(.+?)\*\*', content)
        location = location_match.group(1).strip() if location_match else "æœªçŸ¥"

        # æå–è§’è‰²ï¼ˆæŸ¥æ‰¾æ‰€æœ‰å¯¹è¯å’Œæè¿°ä¸­çš„è§’è‰²åï¼‰
        # ç®€åŒ–å®ç°ï¼šä» state.json è¯»å–å·²çŸ¥è§’è‰²ï¼ŒåŒ¹é…å‡ºç°é¢‘ç‡
        characters = self._extract_characters_from_content(content)

        # è®¡ç®—å­—æ•°
        word_count = len(content)

        # è®¡ç®— Hash
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()

        return {
            'title': title,
            'location': location,
            'characters': characters[:5],  # æœ€å¤š 5 ä¸ªä¸»è¦è§’è‰²
            'word_count': word_count,
            'hash': content_hash
        }

    def _extract_characters_from_content(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–è§’è‰²ï¼ˆç®€åŒ–å®ç°ï¼šè¯»å– state.json å·²çŸ¥è§’è‰²ï¼‰"""

        if not self.state_file.exists():
            return []

        # è¯»å– state.json
        with open(self.state_file, 'r', encoding='utf-8') as f:
            state = json.load(f)

        # è·å–å·²çŸ¥è§’è‰²åˆ—è¡¨
        known_characters = [
            char['name']
            for char in state.get('entities', {}).get('characters', [])
        ]

        # ç»Ÿè®¡æ¯ä¸ªè§’è‰²åœ¨å†…å®¹ä¸­çš„å‡ºç°æ¬¡æ•°
        char_counts = {}
        for char_name in known_characters:
            count = content.count(char_name)
            if count > 0:
                char_counts[char_name] = count

        # æŒ‰å‡ºç°æ¬¡æ•°æ’åºï¼Œè¿”å›å‰ 5 ä¸ª
        sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)
        return [char for char, _ in sorted_chars[:5]]

    # ================== æ ¸å¿ƒåŠŸèƒ½ 2ï¼šä¼ç¬”è¿½è¸ªç´¢å¼• ==================

    def sync_foreshadowing_from_state(self):
        """ä» state.json åŒæ­¥ä¼ç¬”æ•°æ®åˆ°ç´¢å¼•

        è§¦å‘æ—¶æœºï¼š
        - update_state.py æ›´æ–°ä¼ç¬”åè°ƒç”¨
        - --rebuild-index æ‰¹é‡é‡å»ºæ—¶è°ƒç”¨
        """
        if not self.state_file.exists():
            print("âŒ state.json ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¼ç¬”åŒæ­¥")
            return

        # è¯»å– state.json
        with open(self.state_file, 'r', encoding='utf-8') as f:
            state = json.load(f)

        current_chapter = state.get('progress', {}).get('current_chapter', 0)

        # åŒæ­¥æ´»è·ƒä¼ç¬”ï¼ˆæœªå›æ”¶ï¼‰
        active_plots = state.get('plot_threads', {}).get('active', [])
        for plot in active_plots:
            self._index_foreshadowing(plot, current_chapter, status="æœªå›æ”¶")

        # åŒæ­¥å·²å›æ”¶ä¼ç¬”
        resolved_plots = state.get('plot_threads', {}).get('resolved', [])
        for plot in resolved_plots:
            self._index_foreshadowing(plot, current_chapter, status="å·²å›æ”¶")

        self.conn.commit()
        print(f"âœ… ä¼ç¬”ç´¢å¼•å·²åŒæ­¥ï¼š{len(active_plots)} æ¡æ´»è·ƒ + {len(resolved_plots)} æ¡å·²å›æ”¶")

    def _index_foreshadowing(self, plot: Dict, current_chapter: int, status: str):
        """ä¸ºå•ä¸ªä¼ç¬”å»ºç«‹ç´¢å¼•"""

        # è®¡ç®—ç´§æ€¥åº¦
        urgency = self._calculate_urgency(plot, current_chapter)

        # æå–åœ°ç‚¹å’Œè§’è‰²ï¼ˆå¦‚æœæœ‰ï¼‰
        location = plot.get('location', '')
        characters = plot.get('characters', [])

        self.conn.execute("""
            INSERT OR REPLACE INTO foreshadowing_index
            (id, content, location, characters, introduced_chapter, resolved_chapter, status, urgency, updated_at)
            VALUES ((SELECT id FROM foreshadowing_index WHERE content = ?), ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """, (
            plot['description'],  # ç”¨äºæŸ¥é‡
            plot['description'],
            location,
            json.dumps(characters, ensure_ascii=False),
            plot.get('introduced_chapter', 0),
            plot.get('resolved_chapter', None),
            status,
            urgency
        ))

    def _calculate_urgency(self, plot: Dict, current_chapter: int) -> int:
        """è®¡ç®—ä¼ç¬”ç´§æ€¥åº¦ï¼ˆ0-100ï¼‰

        è§„åˆ™ï¼š
        - è¶…è¿‡ 100 ç« æœªå›æ”¶ â†’ æåº¦ç´§æ€¥ï¼ˆ100ï¼‰
        - è¶…è¿‡ 50 ç« æœªå›æ”¶ â†’ ä¸­ç­‰ç´§æ€¥ï¼ˆ60ï¼‰
        - å…¶ä»– â†’ æ­£å¸¸ï¼ˆ20ï¼‰
        """
        introduced_ch = plot.get('introduced_chapter', 0)
        chapters_pending = current_chapter - introduced_ch

        if chapters_pending > 100:
            return 100  # æåº¦ç´§æ€¥
        elif chapters_pending > 50:
            return 60   # ä¸­ç­‰ç´§æ€¥
        else:
            return 20   # æ­£å¸¸

    def query_urgent_foreshadowing(self, threshold: int = 60) -> List[Dict]:
        """æŸ¥è¯¢ç´§æ€¥ä¼ç¬”ï¼ˆurgency >= thresholdï¼‰

        Args:
            threshold: ç´§æ€¥åº¦é˜ˆå€¼ï¼ˆ60=ä¸­ç­‰ç´§æ€¥ï¼Œ80=é«˜åº¦ç´§æ€¥ï¼Œ100=æåº¦ç´§æ€¥ï¼‰

        Returns:
            [{'content': '...', 'introduced_chapter': 45, 'urgency': 80}, ...]
        """
        cursor = self.conn.execute("""
            SELECT content, introduced_chapter, urgency
            FROM foreshadowing_index
            WHERE status = 'æœªå›æ”¶' AND urgency >= ?
            ORDER BY urgency DESC
        """, (threshold,))

        return [dict(row) for row in cursor.fetchall()]

    # ================== æ ¸å¿ƒåŠŸèƒ½ 3ï¼šæ¨¡ç³ŠæŸ¥è¯¢ï¼ˆFuzzy Search via SQL LIKEï¼‰==================

    def fuzzy_search_character(self, keywords: List[str]) -> List[Dict]:
        """æ¨¡ç³ŠæŸ¥è¯¢è§’è‰²ï¼ˆæ”¯æŒå¤šå…³é”®è¯ï¼‰

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚ ["æ", "å¥³å¼Ÿå­"]

        Returns:
            [{'name': 'æé›ª', 'description': '...', 'last_appearance_chapter': 45}, ...]

        ç¤ºä¾‹ï¼š
            fuzzy_search_character(["æ", "å¥³å¼Ÿå­"])
            â†’ è¿”å›æ‰€æœ‰åå­—æˆ–æè¿°åŒ…å«"æ"å’Œ"å¥³å¼Ÿå­"çš„è§’è‰²
        """
        if not self.state_file.exists():
            return []

        # è¯»å– state.json ä¸­çš„è§’è‰²æ•°æ®
        with open(self.state_file, 'r', encoding='utf-8') as f:
            state = json.load(f)

        characters = state.get('entities', {}).get('characters', [])
        matched = []

        for char in characters:
            # æ£€æŸ¥æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½åŒ¹é…
            name = char.get('name', '')
            description = char.get('description', '')
            personality = char.get('personality', '')

            # ç»„åˆæ–‡æœ¬
            combined_text = f"{name} {description} {personality}"

            # æ£€æŸ¥æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½åœ¨ combined_text ä¸­
            if all(keyword in combined_text for keyword in keywords):
                matched.append({
                    'name': name,
                    'description': description,
                    'last_appearance_chapter': char.get('last_appearance_chapter', 0)
                })

        # æŒ‰æœ€åå‡ºåœºç« èŠ‚æ’åº
        matched.sort(key=lambda x: x['last_appearance_chapter'], reverse=True)

        return matched[:10]  # æœ€å¤šè¿”å› 10 ä¸ª

    # ================== æ‰¹é‡æ“ä½œ ==================

    def rebuild_all_indexes(self):
        """æ‰¹é‡é‡å»ºæ‰€æœ‰å†å²ç« èŠ‚çš„ç´¢å¼•

        ä½¿ç”¨åœºæ™¯ï¼š
        - ç´¢å¼•ç³»ç»Ÿé¦–æ¬¡ä¸Šçº¿
        - ç´¢å¼•æ•°æ®åº“æŸå
        """
        if not self.chapters_dir.exists():
            print("âŒ ç« èŠ‚ç›®å½•ä¸å­˜åœ¨")
            return

        # è·å–æ‰€æœ‰ç« èŠ‚æ–‡ä»¶
        chapter_files = sorted(self.chapters_dir.glob("ç¬¬*.md"))

        print(f"ğŸ” å‘ç° {len(chapter_files)} ä¸ªç« èŠ‚æ–‡ä»¶ï¼Œå¼€å§‹é‡å»ºç´¢å¼•...")

        for chapter_file in chapter_files:
            # æå–ç« èŠ‚ç¼–å·
            match = re.search(r'ç¬¬(\d+)ç« ', chapter_file.name)
            if not match:
                continue

            chapter_num = int(match.group(1))

            # é‡å»ºç´¢å¼•
            self._rebuild_chapter_index(chapter_num, chapter_file)

        # åŒæ­¥ä¼ç¬”ç´¢å¼•
        self.sync_foreshadowing_from_state()

        print(f"âœ… æ‰¹é‡é‡å»ºå®Œæˆï¼š{len(chapter_files)} ç« ")

    # ================== æŸ¥è¯¢ä¸ç»Ÿè®¡ ==================

    def get_index_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""

        # ç« èŠ‚ç»Ÿè®¡
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM chapters")
        chapter_count = cursor.fetchone()['count']

        # ä¼ç¬”ç»Ÿè®¡
        cursor = self.conn.execute("""
            SELECT status, COUNT(*) as count
            FROM foreshadowing_index
            GROUP BY status
        """)
        foreshadowing_stats = {row['status']: row['count'] for row in cursor.fetchall()}

        # å…³ç³»ç»Ÿè®¡
        cursor = self.conn.execute("SELECT COUNT(*) as count FROM relationships")
        relationship_count = cursor.fetchone()['count']

        # æ•°æ®åº“å¤§å°
        db_size_kb = self.index_db.stat().st_size / 1024

        return {
            'chapter_count': chapter_count,
            'foreshadowing_active': foreshadowing_stats.get('æœªå›æ”¶', 0),
            'foreshadowing_resolved': foreshadowing_stats.get('å·²å›æ”¶', 0),
            'relationship_count': relationship_count,
            'db_size_kb': round(db_size_kb, 2)
        }

    def __del__(self):
        """ææ„å‡½æ•°ï¼šå…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'conn'):
            self.conn.close()


def main():
    parser = argparse.ArgumentParser(description="ç»“æ„åŒ–ç´¢å¼•ç³»ç»Ÿï¼ˆå–ä»£å‘é‡åŒ–æ£€ç´¢ï¼‰")

    # æ›´æ–°æ“ä½œ
    parser.add_argument("--update-chapter", type=int, metavar="NUM", help="æ›´æ–°å•ç« ç´¢å¼•")
    parser.add_argument("--metadata", metavar="PATH", help="ç« èŠ‚æ–‡ä»¶è·¯å¾„ï¼ˆé…åˆ --update-chapterï¼‰")
    parser.add_argument("--metadata-json", metavar="JSON", help="å…ƒæ•°æ® JSON å­—ç¬¦ä¸²ï¼ˆé…åˆ --update-chapterï¼Œç”± metadata-extractor agent æä¾›ï¼‰")
    parser.add_argument("--metadata-file", metavar="FILE", help="å…ƒæ•°æ® JSON æ–‡ä»¶è·¯å¾„ï¼ˆé…åˆ --update-chapterï¼ŒWindows æ¨èä½¿ç”¨æ­¤å‚æ•°ï¼‰")

    # æ‰¹é‡æ“ä½œ
    parser.add_argument("--rebuild-index", action="store_true", help="æ‰¹é‡é‡å»ºæ‰€æœ‰ç´¢å¼•")

    # æŸ¥è¯¢æ“ä½œ
    parser.add_argument("--query-location", metavar="LOCATION", help="æŸ¥è¯¢åœ°ç‚¹ç›¸å…³ç« èŠ‚")
    parser.add_argument("--query-urgent-foreshadowing", action="store_true", help="æŸ¥è¯¢ç´§æ€¥ä¼ç¬”")
    parser.add_argument("--fuzzy-search", nargs='+', metavar="KEYWORD", help="æ¨¡ç³ŠæŸ¥è¯¢è§’è‰²ï¼ˆå¤šä¸ªå…³é”®è¯ï¼‰")

    # ç»Ÿè®¡ä¿¡æ¯
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯")

    # é¡¹ç›®è·¯å¾„
    parser.add_argument("--project-root", metavar="PATH", help="é¡¹ç›®æ ¹ç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰")

    args = parser.parse_args()

    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index = StructuredIndex(project_root=args.project_root)

    # æ‰§è¡Œæ“ä½œ
    if args.update_chapter:
        # æ¨¡å¼1ï¼šä» JSON æ–‡ä»¶è¯»å–ï¼ˆWindows æ¨èï¼Œé¿å… CLI å¼•å·è½¬ä¹‰é—®é¢˜ï¼‰
        if args.metadata_file:
            try:
                metadata_file = Path(args.metadata_file)
                if not metadata_file.exists():
                    print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
                    return

                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ['title', 'location', 'characters', 'word_count', 'hash']
                missing_fields = [f for f in required_fields if f not in metadata]

                if missing_fields:
                    print(f"âŒ JSON ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
                    return

                # æ›´æ–°ç´¢å¼•
                index.index_chapter(args.update_chapter, metadata)

                # åŒæ­¥ä¼ç¬”ç´¢å¼•
                index.sync_foreshadowing_from_state()

            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æå¤±è´¥: {e}")
                return

        # æ¨¡å¼2ï¼šç›´æ¥æ¥æ”¶ JSON å­—ç¬¦ä¸²ï¼ˆLinux/macOSï¼Œæˆ–æµ‹è¯•æ—¶ä½¿ç”¨ï¼‰
        elif args.metadata_json:
            try:
                metadata = json.loads(args.metadata_json)

                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ['title', 'location', 'characters', 'word_count', 'hash']
                missing_fields = [f for f in required_fields if f not in metadata]

                if missing_fields:
                    print(f"âŒ JSON ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
                    return

                # æ›´æ–°ç´¢å¼•
                index.index_chapter(args.update_chapter, metadata)

                # åŒæ­¥ä¼ç¬”ç´¢å¼•
                index.sync_foreshadowing_from_state()

            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æå¤±è´¥: {e}")
                return

        # æ¨¡å¼3ï¼šä»ç« èŠ‚æ–‡ä»¶æå–å…ƒæ•°æ®ï¼ˆæ—§æ¨¡å¼ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
        elif args.metadata:
            # è¯»å–ç« èŠ‚æ–‡ä»¶
            chapter_file = Path(args.metadata)
            if not chapter_file.exists():
                print(f"âŒ ç« èŠ‚æ–‡ä»¶ä¸å­˜åœ¨: {chapter_file}")
                return

            # æå–å…ƒæ•°æ®
            with open(chapter_file, 'r', encoding='utf-8') as f:
                content = f.read()

            metadata = index._extract_metadata_from_content(content, args.update_chapter)

            # æ›´æ–°ç´¢å¼•
            index.index_chapter(args.update_chapter, metadata)

            # åŒæ­¥ä¼ç¬”ç´¢å¼•
            index.sync_foreshadowing_from_state()

        else:
            print("âŒ ç¼ºå°‘å‚æ•°ï¼š--metadata-file (æ¨è) / --metadata-json / --metadata")
            return

    elif args.rebuild_index:
        index.rebuild_all_indexes()

    elif args.query_location:
        results = index.query_chapters_by_location(args.query_location)

        if not results:
            print(f"æœªæ‰¾åˆ°åœ°ç‚¹ç›¸å…³ç« èŠ‚: {args.query_location}")
        else:
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç« èŠ‚ï¼š")
            for chapter_num, title, characters in results:
                print(f"  Ch{chapter_num}: {title} - è§’è‰²: {characters}")

    elif args.query_urgent_foreshadowing:
        results = index.query_urgent_foreshadowing(threshold=60)

        if not results:
            print("âœ… æ— ç´§æ€¥ä¼ç¬”")
        else:
            print(f"âš ï¸ æ£€æµ‹åˆ° {len(results)} æ¡ç´§æ€¥ä¼ç¬”ï¼š")
            for item in results:
                print(f"  - {item['content'][:30]}...ï¼ˆç¬¬ {item['introduced_chapter']} ç« åŸ‹è®¾ï¼Œç´§æ€¥åº¦ {item['urgency']}/100ï¼‰")

    elif args.fuzzy_search:
        results = index.fuzzy_search_character(args.fuzzy_search)

        if not results:
            print(f"æœªæ‰¾åˆ°åŒ¹é…è§’è‰²: {' + '.join(args.fuzzy_search)}")
        else:
            print(f"æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…è§’è‰²ï¼š")
            for i, char in enumerate(results, 1):
                print(f"{i}. {char['name']} - {char['description'][:50]}...ï¼ˆæœ€åå‡ºåœºï¼šCh {char['last_appearance_chapter']}ï¼‰")

    elif args.stats:
        stats = index.get_index_stats()

        print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"   ç« èŠ‚ç´¢å¼•: {stats['chapter_count']}")
        print(f"   ä¼ç¬”ç´¢å¼•: {stats['foreshadowing_active']} æ¡æ´»è·ƒ + {stats['foreshadowing_resolved']} æ¡å·²å›æ”¶")
        print(f"   å…³ç³»ç´¢å¼•: {stats['relationship_count']}")
        print(f"   æ•°æ®åº“å¤§å°: {stats['db_size_kb']} KB")

    else:
        parser.print_help()


if __name__ == "__main__":
    # Windows UTF-8 ç¼–ç ä¿®å¤ï¼ˆä»…åœ¨è„šæœ¬ç›´æ¥è¿è¡Œæ—¶ï¼‰
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    main()
